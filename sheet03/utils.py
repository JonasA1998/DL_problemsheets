{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-22T18:53:18.574070Z\",\"iopub.execute_input\":\"2022-11-22T18:53:18.574522Z\",\"iopub.status.idle\":\"2022-11-22T18:53:18.594627Z\",\"shell.execute_reply.started\":\"2022-11-22T18:53:18.574475Z\",\"shell.execute_reply\":\"2022-11-22T18:53:18.593445Z\"}}\nimport os\nimport fastprogress\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.utils.data import DataLoader\ndef get_device(cuda_preference=True):\n    \"\"\"Gets pytorch device object. If cuda_preference=True and \n            cuda is available on your system, returns a cuda device.\n\n        Args:\n            cuda_preference: bool, default True\n                Set to true if you would like to get a cuda device\n\n        Returns: pytorch device object\n                Pytorch device\n    \"\"\"\n\n    print('cuda available:', torch.cuda.is_available(), \n          '; cudnn available:', torch.backends.cudnn.is_available(),\n          '; num devices:', torch.cuda.device_count())\n\n    use_cuda = False if not cuda_preference else torch.cuda.is_available()\n    device = torch.device('cuda:0' if use_cuda else 'cpu')\n    device_name = torch.cuda.get_device_name(device) if use_cuda else 'cpu'\n    print('Using device', device_name)\n    return device\ndef grab_data(data_dir, num_cpus=1):\n    \"\"\"Downloads CIFAR10 train and test set, stores them on disk, computes mean \n        and standard deviation per channel of trainset, normalizes the train set\n        accordingly.\n\n    Args:\n        data_dir (str): Directory to store data\n        num_cpus (int, optional): Number of cpus that should be used to \n            preprocess data. Defaults to 1.\n\n    Returns:\n        CIFAR10, CIFAR10, float, float: Returns trainset and testset as\n            torchvision CIFAR10 dataset objects. Returns mean and standard\n            deviation used for normalization.\n    \"\"\"\n    trainset = torchvision.datasets.CIFAR10(data_dir, train=True, download=True, \n                                            transform=torchvision.transforms.ToTensor())\n\n    # Get normalization transform\n    num_samples = trainset.data.shape[0]\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=num_samples, \n                                              num_workers=num_cpus)\n    imgs, _ = next(iter(trainloader))\n    dataset_mean = torch.mean(imgs, dim=(0,2,3))\n    dataset_std = torch.std(imgs, dim=(0,2,3))\n\n    normalized_transform = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(dataset_mean, dataset_std)\n    ])\n\n    # Load again, now normalized\n    trainset = torchvision.datasets.CIFAR10(data_dir, download=True, train=True, \n                                            transform=normalized_transform) \n    # Apply the same transform, computed from the train-set, to the test-set\n    # so both have a similar distribution. We do not normalize the test-set directly,\n    # since we are not allowed to perform any computations with it. (We only use it\n    # for reporting results in the very end)\n    testset = torchvision.datasets.CIFAR10(data_dir, download=True, train=False, \n                                           transform=normalized_transform)\n    return trainset, testset, dataset_mean, dataset_std\ndef generate_train_val_data_split(trainset, split_seed=42, val_frac=0.2):\n    \"\"\"Splits train dataset into train and validation dataset.\n\n    Args:\n        trainset (CIFAR10): CIFAR10 trainset object\n        split_seed (int, optional): Seed used to randomly assign data\n            points to the validation set. Defaults to 42.\n        val_frac (float, optional): Fraction of training set that should be \n            split into validation set. Defaults to 0.2.\n\n    Returns:\n        CIFAR10, CIFAR10: CIFAR10 trainset and validation set.\n    \"\"\"\n    num_val_samples = np.ceil(val_frac * trainset.data.shape[0]).astype(int)\n    num_train_samples = trainset.data.shape[0] - num_val_samples\n    trainset, valset = torch.utils.data.random_split(trainset, \n                                  (num_train_samples, num_val_samples), \n                                  generator=torch.Generator().manual_seed(split_seed))\n    return trainset, valset\n\ndef init_data_loaders(trainset, valset, testset, batch_size=1024, num_cpus=1):\n    \"\"\"Initialize train, validation and test data loader.\n\n    Args:\n        trainset (CIFAR10): Training set torchvision dataset object.\n        valset (CIFAR10): Validation set torchvision dataset object.\n        testset (CIFAR10): Test set torchvision dataset object.\n        batch_size (int, optional): Batchsize that should be generated by \n            pytorch dataloader object. Defaults to 1024.\n        num_cpus (int, optional): Number of CPUs to use when iterating over\n            the data loader. More is faster. Defaults to 1.\n\n    Returns:\n        DataLoader, DataLoader, DataLoader: Returns pytorch DataLoader objects\n            for training, validation and testing.\n    \"\"\"        \n    trainloader = torch.utils.data.DataLoader(trainset,\n                                                   batch_size=batch_size,\n                                                   shuffle=True,\n                                                   num_workers=num_cpus)\n    valloader = torch.utils.data.DataLoader(valset, \n                                                 batch_size=batch_size,\n                                                 shuffle=True,\n                                                 num_workers=num_cpus)\n    testloader = torch.utils.data.DataLoader(testset,\n                                                  batch_size=batch_size,\n                                                  shuffle=True, \n                                                  num_workers=num_cpus)\n    return trainloader, valloader, testloader\ndef accuracy(correct, total): \n    \"\"\"Compute accuracy as percentage.\n\n    Args:\n        correct (int): Number of samples correctly predicted.\n        total (int): Total number of samples\n\n    Returns:\n        float: Accuracy\n    \"\"\"\n    return float(correct)/total\n\n\ndef train(dataloader, optimizer, model, loss_fn, device, master_bar):\n    \"\"\"Run one training epoch.\n\n    Args:\n        dataloader (DataLoader): Torch DataLoader object to load data\n        optimizer: Torch optimizer object\n        model (nn.Module): Torch model to train\n        loss_fn: Torch loss function\n        device (torch.device): Torch device to use for training\n        master_bar (fastprogress.master_bar): Will be iterated over for each\n            epoch to draw batches and display training progress\n\n    Returns:\n        float, float: Mean loss of this epoch, fraction of correct predictions\n            on training set (accuracy)\n    \"\"\"\n    epoch_loss = []\n    epoch_correct, epoch_total = 0, 0\n\n    for x, y in fastprogress.progress_bar(dataloader, parent=master_bar):\n        optimizer.zero_grad()\n        model.train()\n\n        # Forward pass\n        y_pred = model(x.to(device))\n\n        # For calculating the accuracy, save the number of correctly classified \n        # images and the total number\n        epoch_correct += sum(y.to(device) == y_pred.argmax(dim=1))\n        epoch_total += len(y)\n\n        # Compute loss\n        loss = loss_fn(y_pred, y.to(device))\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        # For plotting the train loss, save it for each sample\n        epoch_loss.append(loss.item())\n\n    # Return the mean loss and the accuracy of this epoch\n    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total)\n\n\ndef validate(dataloader, model, loss_fn, device, master_bar):\n    \"\"\"Compute loss, accuracy and confusion matrix on validation set.\n\n    Args:\n        dataloader (DataLoader): Torch DataLoader object to load data\n        model (nn.Module): Torch model to train\n        loss_fn: Torch loss function\n        device (torch.device): Torch device to use for training\n        master_bar (fastprogress.master_bar): Will be iterated over to draw \n            batches and show validation progress\n\n    Returns:\n        float, float, torch.Tensor shape (10,10): Mean loss on validation set, \n            fraction of correct predictions on validation set (accuracy)\n    \"\"\"\n    epoch_loss = []\n    epoch_correct, epoch_total = 0, 0\n    confusion_matrix = torch.zeros(10, 10)    \n\n    model.eval()\n    with torch.no_grad():\n        for x, y in fastprogress.progress_bar(dataloader, parent=master_bar):\n            # make a prediction on validation set\n            y_pred = model(x.to(device))\n\n            # For calculating the accuracy, save the number of correctly \n            # classified images and the total number\n            epoch_correct += sum(y.to(device) == y_pred.argmax(dim=1))\n            epoch_total += len(y)\n\n            # Fill confusion matrix\n            for (y_true, y_p) in zip(y, y_pred.argmax(dim=1)):\n                confusion_matrix[int(y_true), int(y_p)] +=1\n\n            # Compute loss\n            loss = loss_fn(y_pred, y.to(device))\n\n            # For plotting the train loss, save it for each sample\n            epoch_loss.append(loss.item())\n\n    # Return the mean loss, the accuracy and the confusion matrix\n    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), confusion_matrix\n\ndef run_training(model, optimizer, loss_function, device, num_epochs, \n                train_dataloader, val_dataloader, early_stopper=None, verbose=False,scheduler=None):\n    \"\"\"Run model training.\n\n    Args:\n        model (nn.Module): Torch model to train\n        optimizer: Torch optimizer object\n        loss_fn: Torch loss function for training\n        device (torch.device): Torch device to use for training\n        num_epochs (int): Max. number of epochs to train\n        train_dataloader (DataLoader): Torch DataLoader object to load the\n            training data\n        val_dataloader (DataLoader): Torch DataLoader object to load the\n            validation data\n        early_stopper (EarlyStopper, optional): If passed, model will be trained\n            with early stopping. Defaults to None.\n        verbose (bool, optional): Print information about model training. \n            Defaults to False.\n\n    Returns:\n        list, list, list, list, torch.Tensor shape (10,10): Return list of train\n            losses, validation losses, train accuracies, validation accuracies\n            per epoch and the confusion matrix evaluated in the last epoch.\n    \"\"\"\n    start_time = time.time()\n    master_bar = fastprogress.master_bar(range(num_epochs))\n    train_losses, val_losses, train_accs, val_accs = [],[],[],[]\n    if early_stopper:\n        if (early_stopper.path!=None):\n            if(not os.path.isdir(\"/kaggle/working/trained_models/\")):\n                os.chdir(\"/kaggle/working/\")\n                os.mkdir(\"trained_models\")\n            os.chdir(\"/kaggle/working/trained_models/\")\n            if os.path.isdir(early_stopper.path):\n                os.system(\"rm -r \"+early_stopper.path)\n                os.system(\"mkdir \"+early_stopper.path)\n            else:\n                os.system(\"mkdir \"+early_stopper.path)\n            os.chdir(\"/kaggle/working/trained_models/\"+early_stopper.path)\n    for epoch in master_bar:\n        # Train the model\n        epoch_train_loss, epoch_train_acc = train(train_dataloader, optimizer, model, \n                                                  loss_function, device, master_bar)\n        # Validate the model\n        epoch_val_loss, epoch_val_acc, confusion_matrix = validate(val_dataloader, \n                                                                   model, loss_function, \n                                                                   device, master_bar)\n        if scheduler:\n            if isinstance(scheduler,torch.optim.lr_scheduler.ReduceLROnPlateau):\n                scheduler.step(epoch_val_loss)\n            else:\n                scheduler.step()\n\n        # Save loss and acc for plotting\n        train_losses.append(epoch_train_loss)\n        val_losses.append(epoch_val_loss)\n        train_accs.append(epoch_train_acc)\n        val_accs.append(epoch_val_acc)\n        \n        if verbose:\n            master_bar.write(f'Train loss: {epoch_train_loss:.2f}, val loss: {epoch_val_loss:.2f}, train acc: {epoch_train_acc:.3f}, val acc {epoch_val_acc:.3f}')\n            \n        if early_stopper:\n            early_stopper.val_acc.append(epoch_val_acc)\n            early_stopper.check()\n            if (early_stopper.path!=None):\n                torch.save(model.state_dict(),\"model_for_epoch_\"+str(len(early_stopper.val_acc)))\n            if(early_stopper.early_stop):\n                if (early_stopper.path!=None):\n                    model.load_state_dict(torch.load(\"model_for_epoch_\"+str(len(early_stopper.val_acc)-early_stopper.patience)))\n                break\n            #raise NotImplementedError # Comment out this keyword after your implementation\n            \n            # END OF YOUR CODE #\n            \n    time_elapsed = np.round(time.time() - start_time, 0).astype(int)\n    print(f'Finished training after {time_elapsed} seconds.')\n    return train_losses, val_losses, train_accs, val_accs, confusion_matrix\n\n\ndef plot(title, label, train_results, val_results, yscale='linear', save_path=None, \n         extra_pt=None, extra_pt_label=None):\n    \"\"\"Plot learning curves.\n\n    Args:\n        title (str): Title of plot\n        label (str): x-axis label\n        train_results (list): Results vector of training of length of number\n            of epochs trained. Could be loss or accuracy.\n        val_results (list): Results vector of validation of length of number\n            of epochs. Could be loss or accuracy.\n        yscale (str, optional): Matplotlib.pyplot.yscale parameter. \n            Defaults to 'linear'.\n        save_path (str, optional): If passed, figure will be saved at this path.\n            Defaults to None.\n        extra_pt (tuple, optional): Tuple of length 2, defining x and y coordinate\n            of where an additional black dot will be plotted. Defaults to None.\n        extra_pt_label (str, optional): Legend label of extra point. Defaults to None.\n    \"\"\"\n    \n    epoch_array = np.arange(len(train_results)) + 1\n    train_label, val_label = \"Training \"+label.lower(), \"Validation \"+label.lower()\n    \n    sns.set(style='ticks')\n\n    plt.plot(epoch_array, train_results, epoch_array, val_results, linestyle='dashed', marker='o')\n    legend = ['Train results', 'Validation results']\n    \n    if extra_pt:\n        ####################\n        ## YOUR CODE HERE ##\n        ####################\n        raise NotImplementedError # Comment out this keyword after your implementation\n\n        # END OF YOUR CODE #\n        \n    plt.legend(legend)\n    plt.xlabel('Epoch')\n    plt.ylabel(label)\n    plt.yscale(yscale)\n    plt.title(title)\n    \n    sns.despine(trim=True, offset=5)\n    plt.title(title, fontsize=15)\n    if save_path:\n        plt.savefig(str(save_path), bbox_inches='tight')\n    plt.show()\n\n# %% [code]\n","metadata":{"_uuid":"20fd676b-8d95-4cf6-9a4e-db5b15f5a6d6","_cell_guid":"1ccd362f-21db-4cca-bf2a-14d2aac70120","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_27/159599803.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (159599803.py, line 1)","output_type":"error"}]}]}